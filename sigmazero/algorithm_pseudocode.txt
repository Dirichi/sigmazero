- sigmazero algorithm
	- self-play
		- at each time step:
			- pass observation o into representation function to get hidden state representation s
			- pass state s into prediction function to get prior action distribution and value of root node
			- for a number of simulations:
				- traverse tree based on maximizing UCB score until you end up at a leaf set
				- pass parent set hidden state representation s and corresponding action vector a into dynamics function g(s,a) and get:
					- the outputted hidden state representation of all possible future outcomes for all parent nodes
					- the predicted transition probabilities for all possible future outcomes for all parent nodes 
					- the transition rewards for all possible future outcomes occurring for all parent nodes
				- pass the outputted child set hidden state representation s' into the prediction function f(s) and get:
					- the predicted prior action distribution for each child node
					- the predicted value for each child node
				- assign child_set attributes:
					- .branching_factor_indices: calculate this using the parent_set.branching_factor_indices and the branching_factor hyperparameter in the config file
					- .hidden_state_representation_matrix = output of the dynamics function
					- .transition_probabilities: output of the dynamics function, multiplied by the corresponding parent_node.transition_probabilities
					- .expected_transition_reward: output of the dynamics function, multiplied by the corresponding child_node.transition_probabilities
					- .expected_prior_action_distribution: output of the prediction function, multiplied by the corresponding child_node.transition_probabilities
					- .expected_value: output of the prediction function, multiplied by the corresponding child_node.transition_probabilities
				- set v = child_set.expected_value
				- for every node_set in the search path (starting from the bottom-most level):
					- node_set.cumulative_expected_value += v
					- node_set.num_visits += 1
					- v = node_set.expected_transition_reward + discount_factor * v
					- update min/max values for normalizing values, if node_set.cumulative_expected_value / node_set.num_visits exceeds the boundaries
			- sample an action based on visit counts of the child set of the root set
	- training
		- sample a game trajectory of a finite length
			- each sequence in the trajectory contains a state s_t, an action a_t applied to s_t, a resulting transition reward r_{t+1}, the MCTS predicted policy pi_t and the MCTS predicted value v_t
		- pass the first observation o_1 into the representation function to get the hidden state representation s_1
		- assign the current state_set = [s_1]
		- for every time step t in the game trajectory:
			- apply action a_t to state_set by passing both values into the dynamics function g(state_set,a_t) and get:
				- the outputted hidden state representation of all possible future outcomes for all parent nodes
				- the predicted transition probabilities for all possible future outcomes for all parent nodes 
				- the transition rewards for all possible future outcomes occurring for all parent nodes
			- pass the outputted child hidden state representation s' into the prediction function f(s) and get:
				- the predicted prior action distribution for each child node
				- the predicted value for each child node
			- assign child_set attributes:
				- .branching_factor_indices: calculate this using the parent_set.branching_factor_indices and the branching_factor hyperparameter in the config file
				- .hidden_state_representation_matrix = output of the dynamics function
				- .transition_probabilities: output of the dynamics function, multiplied by the corresponding parent_node.transition_probabilities
				- .expected_transition_reward: output of the dynamics function, multiplied by the corresponding child_node.transition_probabilities
				- .expected_prior_action_distribution: output of the prediction function, multiplied by the corresponding child_node.transition_probabilities
				- .expected_value: output of the prediction function, multiplied by the corresponding child_node.transition_probabilities
			- set current_state_set = child_set.hidden_state_representation_matrix
			- train with the following target and prediction matching:
				- MCTS predicted policy pi_t and the predicted prior action distribution outputted by the prediction function
				- actual transition reward received r_{t+1} and the predicted expected transition reward current_state_set.expected_transition_reward, calculated from the dynamics function
				- calculated value through n-step bootstrapping (using a combination of actual transition reward received and the MCTS predicted value v_t), and the predicted expected value current_state_set.expected_value, calculated from the prediction function
	- a node_set contains all stochastic game outcomes given the action sequences so far
		- therefore its expected transition reward, expected value and expected prior action distribution, is calculated using a weighted sum of all the nodes in its set
		- when branching_factor hyperparameter is set to 1, then the node_set only contains one node, and sigmazero reduces to muzero
			- with branching_factor = 1, the dynamics function will always output a transition probability of 1 for the new hidden state, because of its softmax activation function
		- the only difference in mcts between muzero and sigmazero is the node expansion
			- in muzero, node expansion deals with scalar values
			- in sigmazero, node set expansion deals with matrices, that grow in size at every increasing depth level
		- backpropagation is the same in both muzero and sigmazero, as we backpropagate up a scalar value and an expected value, respectively
		- we are essentially calculating expected value, transition_reward and prior action distribution for sigmazero because we're dealing with stochastic environments
			- compared to just singular value, transition_reward and prior action distribution for muzero, since the dynamics are deterministic
	- implement node set attributes as matrices:
		- .hidden_state_representation_matrix: ( (nodes x branching_factor) x features )
			- passing this into the dynamics function g( ( (nodes x branching_factor) x features ) , ( (nodes x branching_factor) x action_vector ) results in:
				- child_set_hidden_state_representation: ( (nodes x branching_factor**2) x features)
				- transition_probabilities: ( nodes x branching_factor**2 )
				- transition_rewards: ( nodes x branching_factor**2 )
		- .branching_factor_indices: list[int]
			- if we partition .hidden_state_representation_matrix using these indices along the (nodes x branching_factor) axis, 
				- we get the group of nodes that share the same parent node; i.e. the child nodes all branched from the result of applying a specific action to the same parent
		- .transition_probabilities: ( nodes x branching_factor ) vector
			- the TOTAL transition probability to get to that particular node set
			- this value needs to factor in the transition probabilities of all ancestor sets when this node set is first expanded
				- i.e. multiply all transition probabilities of the ancestor sets
		- .expected_transition_reward: float
		- .expected_prior_action_distribution: (action_indices) vector
		- .expected_value: float
		- .cumulative_expected_value: float
		- .num_visits: int